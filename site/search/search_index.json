{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Vinagent","text":"<p><code>Vinagent</code> is a simple and flexible library designed for building smart agent assistants across various industries. Vinagent towards the AI in multiple-industries, distinguishing with other agent library by its simplicity, integrability, observability, and optimizablity.</p> <p>Whether you're creating an AI-powered customer service bot, a data analysis assistant, or a domain-specific automation agent, vinagent provides a simple yet powerful foundation.</p> <p>With its modular tool system, you can easily extend your agent's capabilities by integrating a wide range of tools. Each tool is self-contained, well-documented, and can be registered dynamically\u2014making it effortless to scale and adapt your agent to new tasks or environments.</p>"},{"location":"#extraordinary-benefits","title":"Extraordinary benefits","text":"<p><code>Vinagent</code> helps design AI Agents to solve various tasks across multiple fields such as Financial and Banking, Healthcare, Manufacturing, and Autonomous Systems. The strength of Vinagent lies in its focus on a library for designing agents with simple syntax and upgrading agents with all the components of a modern Agent design. What do these designs have to make Vinagent different?</p> <ul> <li> <p>Tools: Supports a variety of different tools, from user-implemented tools like Function tool and Module tool, to tools from the MCP market. Thus, Vinagent ensures you always have all the necessary features and data for every task.</p> </li> <li> <p>Memory: Vinagent organizes the short-term and long-term memory of the Agent through graph storage, creating a graph network that compresses information more efficiently than traditional conversation history storage. This innovative approach helps Vinagent save memory and minimize hallucination.</p> </li> <li> <p>Planning and Control: Based on the graph foundation of Langgraph, Vinagent designs workflows with simpler syntax, using the right shift <code>&gt;&gt;</code> operator, which is easy to use for beginers. This makes creating and managing complex workflows much simpler compared to other agent workflow libraries, even for a complex conditional and parallel workflows.</p> </li> <li> <p>Improving user Experience: Vinagent supports inference through three methods: asynchronous, synchronous, and streaming. This flexibility allows you to speed up processing and improve user experience when applying agents in AI products that require fast and immediate processing speeds.</p> </li> <li> <p>Prompt Optimization: Vinagent integrates automatic prompt optimization features, enhancing accuracy for Agents. This ensures the Agent operates effectively even with specialized tasks. Observability: Allows monitoring of the Agent\u2019s processing information on-premise and is compatible with Jupyter Notebook. You can measure total processing time, the number of input/output tokens, as well as LLM model information at each step in the workflow. This detailed observability feature is crucial for debugging and optimizing the Agent.</p> </li> </ul>"},{"location":"#vinagent-ecosystem","title":"Vinagent ecosystem","text":"<p>Although <code>Vinagent</code> can stand as a independent library for agent, it is designed to be integrated with other Vinagent's Ecosystem libraries that expands its capabilities rather than just a simple Agent. The <code>Vinagent</code> ecosystem consists of the following components:</p> <ul> <li> <p>Aucodb: An open-source database for storing and managing data for AI Agent, providing a flexible solution for storing and retrieving data for Vinagent's agents under multiple format such as collection of tools, messages, graph, vector storage, and logging. Aucodb can ingest and transform various text data into knowledge graph and save to neo4j and adapt various popular vector store databases like <code>chroma, faiss, milvus, pgvector, pinecone, qdrant, and weaviate</code>.</p> </li> <li> <p>Mlflow - Extension: Intergate with mlflow library to log the Agent's information and profile the Agent's performance. This allows you to track the Agent capability and optimize.</p> </li> </ul>"},{"location":"contributing/contributing/","title":"Contributing","text":""},{"location":"get_started/add_memory/","title":"Agent memory","text":"<p><code>Vinagent</code> features a <code>Graphical Memory</code> system that transforms messages into a structured knowledge graph composed of nodes, relationships, and edges. This memory can be organized into short-term and long-term components, allowing the Agent to retain and recall learned information effectively.</p> <p>Compared to traditional <code>Conversational Memory, Graphical Memory</code> offers distinct advantages: it condenses essential information into a graph format, reducing hallucinations by filtering out redundant or irrelevant details. Additionally, because it operates with a shorter context length, it significantly lowers computational costs.</p> <p>This graph-based approach mirrors how humans conceptualize and retain knowledge, making it especially powerful for capturing the core meaning of complex conversations.</p>"},{"location":"get_started/add_memory/#setup","title":"Setup","text":"<p>Install <code>vinagent</code> package</p> <pre><code>%pip install vinagent\n</code></pre> <p>Write environment variable</p> <pre><code>%%writefile .env\nTOGETHER_API_KEY=\"Your together API key\"\nTAVILY_API_KEY=\"Your Tavily API key\"\n</code></pre>"},{"location":"get_started/add_memory/#initialize-memory","title":"Initialize Memory","text":"<p>Vinagent is outstanding with organizing Memory as a Knowledge Graph. We leverage <code>AucoDB</code> features to enhance graph's capabilities of agent. The graph-based features is supported as follows:</p> <ul> <li> <p>Graph Construction: Builds knowledge graphs from documents using <code>LLMGraphTransformer</code> class from AucoDB, extracting entities (nodes) and relationships with enriched properties such as categories, roles, or timestamps.</p> </li> <li> <p>Property Enrichment: Enhances nodes and relationships with contextual attributes derived from the input text, improving graph expressiveness.</p> </li> <li> <p>Graph Visualization: Visualizes the constructed graph and exports it as a html file for easy sharing and analysis.</p> </li> <li> <p>Neo4j Integration: Stores and manages graphs in a Neo4j database with secure client initialization.</p> </li> <li> <p>Flexible Input: Processes unstructured text to create structured graphs, suitable for applications like knowledge management, AI research, and data analysis.</p> </li> </ul> <p>Prerequisites</p> <ul> <li>Neo4j Database: A running Neo4j instance (local or remote).</li> <li>Python Packages: Install required dependencies:</li> </ul> <pre><code>pip install langchain-together neo4j python-dotenv\n</code></pre> <pre><code>from langchain_together.chat_models import ChatTogether\nfrom dotenv import load_dotenv, find_dotenv\nfrom vinagent.memory import Memory\n\nload_dotenv(find_dotenv('.env'))\n\nllm = ChatTogether(\n    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\"\n)\n\nmemory = Memory(\n    memory_path=\"templates/memory.jsonl\",\n    is_reset_memory=True, # will reset the memory every time the agent is invoked\n    is_logging=True\n)\n</code></pre> <pre><code>text_input = \"\"\"Hi, my name is Kan. I was born in Thanh Hoa Province, Vietnam, in 1993.\nMy motto is: \"Make the world better with data and models\". That\u2019s why I work as an AI Solution Architect at FPT Software and as an AI lecturer at NEU.\nI began my journey as a gifted student in Mathematics at the High School for Gifted Students, VNU University, where I developed a deep passion for Math and Science.\nLater, I earned an Excellent Bachelor's Degree in Applied Mathematical Economics from NEU University in 2015. During my time there, I became the first student from the Math Department to win a bronze medal at the National Math Olympiad.\nI have been working as an AI Solution Architect at FPT Software since 2021.\nI have been teaching AI and ML courses at NEU university since 2022.\nI have conducted extensive research on Reliable AI, Generative AI, and Knowledge Graphs at FPT AIC.\nI was one of the first individuals in Vietnam to win a paper award on the topic of Generative AI and LLMs at the Nvidia GTC Global Conference 2025 in San Jose, USA.\nI am the founder of DataScienceWorld.Kan, an AI learning hub offering high-standard AI/ML courses such as Build Generative AI Applications and MLOps \u2013 Machine Learning in Production, designed for anyone pursuing a career as an AI/ML engineer.\nSince 2024, I have participated in Google GDSC and Google I/O as a guest speaker and AI/ML coach for dedicated AI startups.\n\"\"\"\n\nmemory.save_short_term_memory(llm, text_input, user_id=\"Kan\")\nmemory_message = memory.load_memory_by_user(load_type='string', user_id=\"Kan\")\nprint(memory_message)\n</code></pre> <pre><code>Kan -&gt; BORN_IN[in 1993] -&gt; Thanh Hoa Province, Vietnam\nKan -&gt; WORKS_FOR[since 2021] -&gt; FPT Software\nKan -&gt; WORKS_FOR[since 2022] -&gt; NEU\nKan -&gt; STUDIED_AT -&gt; High School for Gifted Students, VNU University\nKan -&gt; STUDIED_AT[graduated in 2015] -&gt; NEU University\nKan -&gt; RESEARCHED_AT -&gt; FPT AIC\nKan -&gt; RECEIVED_AWARD[at Nvidia GTC Global Conference 2025] -&gt; paper award on Generative AI and LLMs\nKan -&gt; FOUNDED -&gt; DataScienceWorld.Kan\nKan -&gt; PARTICIPATED_IN[since 2024] -&gt; Google GDSC\nKan -&gt; PARTICIPATED_IN[since 2024] -&gt; Google I/O\nDataScienceWorld.Kan -&gt; OFFERS -&gt; Build Generative AI Applications\nDataScienceWorld.Kan -&gt; OFFERS -&gt; MLOps \u2013 Machine Learning in Production\n</code></pre>"},{"location":"get_started/add_memory/#load-memory-by-user_id","title":"Load memory by user_id","text":"<p>Memory is organized by <code>user_id</code> to segment each user\u2019s data within the long-term memory. Before starting a conversation, the agent can access the memory associated with the given <code>user_id</code>, which helps prevent confusion between users the agent has previously interacted with and toward on a more personalized experience. For instance, if the agent has a conversation with Mr. Kan, it can recall all that information in future sessions by referencing <code>user_id='Kan'</code>.</p> <pre><code>message_user = memory.load_memory_by_user(load_type='list', user_id=\"Kan\")\n</code></pre> <pre><code>message_user\n</code></pre> <pre><code>[{'head': 'Kan',\n  'head_type': 'Person',\n  'relation': 'BORN_IN',\n  'relation_properties': 'in 1993',\n  'tail': 'Thanh Hoa Province, Vietnam',\n  'tail_type': 'Location'},\n {'head': 'Kan',\n  'head_type': 'Person',\n  'relation': 'WORKS_FOR',\n  'relation_properties': 'since 2021',\n  'tail': 'FPT Software',\n  'tail_type': 'Company'},\n {'head': 'Kan',\n  'head_type': 'Person',\n  'relation': 'WORKS_FOR',\n  'relation_properties': 'since 2022',\n  'tail': 'NEU',\n  'tail_type': 'University'},\n  ...\n]\n</code></pre> <p>Therefore, Agent can utilize this personalized graph-based memory to provide more accurate and relevant responses, which align with user's preferences.</p>"},{"location":"get_started/add_memory/#visualize-on-neo4j","title":"Visualize on Neo4j","text":"<p>You can explore the knowledge graph on-premise using the Neo4j dashboard at <code>http://localhost:7474/browser/</code>. This allows you to intuitively understand the nodes and relationships within your data.</p> <p>To enable this visualization, the <code>AucoDBNeo4jClient</code>, a client instance from the <code>AucoDB</code> library, supports ingesting graph memory directly into a <code>Neo4j</code> database. Once the data is ingested, you can use <code>Cypher</code> queries to retrieve nodes and edges for inspection or further analysis.</p> <p>Note</p> <p>Authentication: Access to the Neo4j dashboard requires login using the same <code>username/password</code> credentials configured in your Docker environment (e.g., via the NEO4J_AUTH variable).</p> <p>If you prefer not to use the <code>Neo4j</code> web interface, the <code>AucoDBNeo4jClient</code> also provides a convenient method to export the entire graph to an HTML file, which is <code>client.visualize_graph(output_path=\"my_graph.html\")</code>.</p> <p>This method generates a standalone HTML file containing an interactive graph visualization, ideal for embedding in reports or sharing with others without requiring Neo4j access.</p>"},{"location":"get_started/add_memory/#start-neo4j-service","title":"Start Neo4j service","text":"<p>Neo4j database can be install as a docker service. We need to create a <code>docker-compose.yml</code> file on local and start <code>Neo4j</code> database as follows:</p> <pre><code>%%writefile docker-compose.yml\nversion: '3.8'\n\nservices:\n  neo4j:\n    image: neo4j:latest\n    container_name: neo4j\n    ports:\n      - \"7474:7474\"\n      - \"7687:7687\"\n    environment:\n      - NEO4J_AUTH=neo4j/abc@12345\n</code></pre> <p>Start <code>neo4j</code> service by command:</p> <pre><code>docker-compose up\n</code></pre>"},{"location":"get_started/add_memory/#export-knowledge-graph","title":"Export Knowledge Graph","text":"<p>Install dependency <code>AucoDB</code> library to ingest knowledge graph to <code>Neo4j</code> database and and export graph to html file.</p> <pre><code>%pip install aucodb\n</code></pre> <p>Initialze client instance</p> <pre><code>from langchain_together.chat_models import ChatTogether\nfrom aucodb.graph.neo4j_client import AucoDBNeo4jClient\nfrom aucodb.graph import LLMGraphTransformer\nfrom dotenv import load_dotenv\n\n# Step 1: Initialize AucoDBNeo4jClient\n# Method 1: dirrectly passing arguments, but not ensure security\nNEO4J_URI = \"bolt://localhost:7687\"  # Update with your Neo4j URI\nNEO4J_USER = \"neo4j\"  # Update with your Neo4j username\nNEO4J_PASSWORD = \"abc@12345\"  # Update with your Neo4j password\n\nclient = AucoDBNeo4jClient(uri = NEO4J_URI, user = NEO4J_USER, password = NEO4J_PASSWORD)\n</code></pre> <pre><code># Step 2: Save user memory into jsonline\nimport json\n\nwith open(\"user_memory.jsonl\", \"w\") as f:\n    for item in message_user:\n        f.write(json.dumps(item) + \"\\n\")\n</code></pre> <pre><code># Step 3: Load user_memory.jsonl into Neo4j.\nclient.load_json_to_neo4j(\n    json_file='user_memory.jsonl',\n    is_reset_db=False\n)\n</code></pre> <pre><code># Step 4: Export graph into my_graph.html file.\nclient.visualize_graph(output_file=\"my_graph.html\", show_in_browser=True)\n</code></pre> <p></p>"},{"location":"get_started/add_tool/","title":"Add tools","text":""},{"location":"get_started/add_tool/#prerequisites","title":"Prerequisites","text":"<p>Install <code>vinagent</code> library</p> <pre><code>%pip install vinagent\n</code></pre>"},{"location":"get_started/add_tool/#tool-types","title":"Tool types","text":"<p><code>Vinagent</code> allows you to connect to three types of tools:</p> <ul> <li>Function tool: A Python function is registered into a specific agent using a decorator.</li> <li>Module tool: A function from a Python module, saved in a specific folder, can be registered as a tool.</li> <li>MCP tool: Create an MCP tool, which connects to an MCP server using the MCP protocol.</li> </ul>"},{"location":"get_started/add_tool/#example-of-module-tool","title":"Example of module tool","text":"<p>You can add module tools from a Python module path as follows: - Initialize an LLM model, which can be any model wrapped by the Langchain BaseLLM class. I use TogetherAI chat model in there, thus, you need to create <code>.env</code> environment with variable <pre><code>TOGETHER_API_KEY=\"Your together API key\"\n</code></pre> You can use other LLM Provider API as long as it was initialized by Langchain <code>BaseLLM</code> class.</p> <pre><code>from langchain_together import ChatTogether \nfrom vinagent.agent.agent import Agent\nfrom dotenv import load_dotenv, find_dotenv\nload_dotenv(find_dotenv('.env'))\n\nllm = ChatTogether(\n    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\"\n)\n</code></pre> <ul> <li>Initialize an Agent with tools, which are wrapped inside the tools argument as a list of paths:</li> </ul> <pre><code>import os\nos.makedirs('./tools', exist_ok=True)\n</code></pre> <pre><code>%%writefile tools/hello.py\ndef hello_from_vinagent():\n    '''A greet of Vinagent to everyone'''\n    return \"Hello my cute cute friend, I'm vinagent and I am here to play with you \ud83d\ude04!\"\n</code></pre> <pre><code>Writing tools/hello.py\n</code></pre> <pre><code># Step 1: Create Agent with tools\nagent = Agent(\n    description=\"You are a Vinagent\",\n    llm = llm,\n    skills = [\n        \"Friendly talk with anyone\"\n    ],\n    tools = ['tools/hello.py'],\n    tools_path = 'templates/tools.json',\n    is_reset_tools = True\n)\n</code></pre> <pre><code>INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\nINFO:vinagent.register.tool:Registered hello_from_vinagent:\n{'tool_name': 'hello_from_vinagent', 'arguments': {}, 'return': \"Hello my cute cute friend, I'm vinagent and I am here to play with you \ud83d\ude04!\", 'docstring': 'A greet of Vinagent to everyone', 'dependencies': [], 'module_path': 'vinagent.tools.hello', 'tool_type': 'module', 'tool_call_id': 'tool_a25e45c3-81df-4b68-982d-d308c403a725'}\nINFO:vinagent.register.tool:Completed registration for module vinagent.tools.hello\n</code></pre> <p>Note</p> <p><code>tools_path</code> is where the tools dictionary is saved. The default value is templates/tools.json.</p> <p>Resetting Your Tools</p> <p>If you set <code>is_reset_tools = True</code>, it will override the tool definitions every time an agent is reinitialized.</p>"},{"location":"get_started/add_tool/#asking-tool","title":"Asking tool","text":"<pre><code># Step 2: invoke the agent\nmessage = agent.invoke(\"Hi Vinagent, Can you greet by your style?\")\nprint(message.content)\n</code></pre> <pre><code>Hello my friend, I'm vinagent, an AI smart assistant. I come here to help you \ud83d\ude04!\n</code></pre>"},{"location":"get_started/async_invoke/","title":"Asynchronous Invoke","text":""},{"location":"get_started/async_invoke/#prerequisites","title":"Prerequisites","text":"<pre><code>%pip install vinagent\n</code></pre>"},{"location":"get_started/async_invoke/#initialize-llm-and-agent","title":"Initialize LLM and Agent","text":"<p>To use a list of default tools inside vinagent.tools you should set environment varibles inside <code>.env</code> including <code>TOGETHER_API_KEY</code> to use llm models at togetherai site and <code>TAVILY_API_KEY</code> to use tavily websearch tool at tavily site:</p> <pre><code>%%writefile .env\nTOGETHER_API_KEY=\"Your together API key\"\nTAVILY_API_KEY=\"Your Tavily API key\"\n</code></pre> <pre><code>from vinagent.agent.agent import Agent\nfrom langchain_together import ChatTogether\nfrom dotenv import load_dotenv, find_dotenv\n\nload_dotenv(find_dotenv('.env'))\n\n# Step 1: Initialize LLM\nllm = ChatTogether(\n    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\"\n)\n\n# Step 2: Initialize Agent\nagent = Agent(\n    description=\"You are a Weather Analyst\",\n    llm = llm,\n    skills = [\n        \"Update weather at anywhere\",\n        \"Forecast weather in the futher\",\n        \"Recommend picnic based on weather\"\n    ],\n    tools=['vinagent.tools.websearch_tools'],\n    tools_path = 'templates/tools.json', # Place to save tools. Default is 'templates/tools.json'\n    is_reset_tools = True # If True, it will reset tools every time reinitializing an agent. Default is False\n)\n</code></pre> <pre><code>INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\nINFO:vinagent.register.tool:Registered search_api:\n{'tool_name': 'search_api', 'arguments': {'query': {'type': 'Union[str, dict[str, str]]', 'value': '{}'}}, 'return': 'Any', 'docstring': 'Search for an answer from a query string\\n    Args:\\n        query (dict[str, str]):  The input query to search\\n    Returns:\\n        The answer from search query', 'dependencies': ['os', 'dotenv', 'tavily', 'dataclasses', 'typing'], 'module_path': 'vinagent.tools.websearch_tools', 'tool_type': 'module', 'tool_call_id': 'tool_d697f931-5c00-44cf-b2f1-f70f91cc2973'}\nINFO:vinagent.register.tool:Completed registration for module vinagent.tools.websearch_tools\n</code></pre>"},{"location":"get_started/async_invoke/#syntax-for-async-invoke","title":"Syntax for Async Invoke","text":"<p>Vinagent supports both synchronous (<code>agent.invoke</code>) and asynchronous (<code>agent.ainvoke</code>) execution methods. Synchronous calls block the main thread until a response is received, whereas asynchronous calls allow the program to continue running while waiting for a response. This makes asynchronous execution especially effective for I/O-bound tasks, such as when interacting with external services like search engine, database connection, weather API, .... In real-world usage, asynchronous calls can perform up to twice as fast as their synchronous counterparts.</p> <pre><code>message = await agent.ainvoke(\"What is the weather in New York today?\")\nprint(message.content)\n</code></pre>"},{"location":"get_started/async_invoke/#latency-benchmarking","title":"Latency Benchmarking","text":"<p>This is a performance benchmarking table based on 100 requests to meta-llama/Llama-3.3-70B-Instruct-Turbo-Free on TogetherAI. It demonstrates that the latency of <code>ainvoke</code> is nearly twice as fast as <code>invoke</code>. You may get different results due to the randomness of the requests and state of LLM-provider server.</p> Number of requests <code>invoke</code> (sec/req) <code>ainvoke</code> (req/req) 100 8.05-11.72 15.03-18.47 <p>This is code for benchmarking between two inference methods. To save cost, we only run 5 times.</p> <p><pre><code>import timeit\nimport asyncio\n\nasync def benchmark_ainvoke():\n    message = await agent.ainvoke(\"What is the weather in New York today?\")\n    print(message.content)\n    return message\n\ndef sync_wrapper():\n    asyncio.run(benchmark_ainvoke())\n\n\nexecution_time = timeit.timeit(sync_wrapper, number=5)\nprint(f\"Average execution of asynchronous time over 5 runs: {execution_time / 5:.2f} seconds\")\n</code></pre>     Average execution of asynchronous time over 5 runs: 8.93 seconds</p> <p><pre><code>import timeit\n\ndef benchmark_invoke():\n    message = agent.invoke(\"What is the weather in New York today?\")\n    print(message.content)\n\nexecution_time = timeit.timeit(benchmark_invoke, number=5)\nprint(f\"Average execution of synchronous time over 5 runs: {execution_time / 5:.2f} seconds\")\n</code></pre>     Average execution of synchronous time over 5 runs: 15.47 seconds</p>"},{"location":"get_started/basic_agent/","title":"Build a basic Chatbot","text":"<p>This tutorial introduce you how to create a simple Agent with minimal components and how to use them. This offers a general view on agent initialization and tool integration.</p>"},{"location":"get_started/basic_agent/#installation","title":"Installation","text":"<p>The python distribution version of Vinagent library is avaible on pypi.org channel and github, which facilitates the installation of the library.</p> <p>Dev version on git</p> <p>You can clone git repository and install by poetry command. This is suitable to obtain the latest development version.</p> <pre><code>git@github.com:datascienceworld-kan/vinagent.git\ncd vinagent\npip install -r requirements.txt\npoetry install\n</code></pre> <p>Stable version</p> <p>You can install the stable distributed versions which are tested and distributed on pypi.org channel by pip command</p> <pre><code>pip install vinagent=='Put_Your_Version'\n</code></pre>"},{"location":"get_started/basic_agent/#prerequisites","title":"Prerequisites","text":"<p>To use a list of default tools inside vinagent.tools you should set environment varibles inside <code>.env</code> including <code>TOGETHER_API_KEY</code> to use llm models at togetherai site and <code>TAVILY_API_KEY</code> to use tavily websearch tool at tavily site:</p> <p><pre><code>TOGETHER_API_KEY=\"Your together API key\"\nTAVILY_API_KEY=\"Your Tavily API key\"\n</code></pre> Let's create your acounts first and then create your relevant key for each website.</p>"},{"location":"get_started/basic_agent/#setup-an-agent","title":"Setup an Agent","text":"<p><code>vinagent</code> is a flexible library for creating intelligent agents. You can configure your agent with tools, each encapsulated in a Python module under <code>vinagent.tools</code>. This provides a workspace of tools that agents can use to interact with and operate in the realistic world. Each tool is a Python file with full documentation and it can be independently ran. For example, the vinagent.tools.websearch_tools module contains code for interacting with a search API.</p> <pre><code>from langchain_together import ChatTogether \nfrom vinagent.agent.agent import Agent\nfrom dotenv import load_dotenv\nload_dotenv()\n\nllm = ChatTogether(\n    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\"\n)\n\n# Step 1: Create Agent with tools\nagent = Agent(\n    description=\"You are a Financial Analyst\",\n    llm = llm,\n    skills = [\n        \"Deeply analyzing financial markets\", \n        \"Searching information about stock price\",\n        \"Visualization about stock price\"]\n)\n\n# Step 2: invoke the agent\nmessage = agent.invoke(\"Who you are?\")\nprint(message)\n</code></pre> <p>If the answer is a normal message without using any tools, it will be an <code>AIMessage</code>. By contrast, it will have <code>ToolMessage</code> type. For examples:</p> <p><pre><code>AIMessage(content='I am a Financial Analyst.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 308, 'total_tokens': 315, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'cached_tokens': 0}, 'model_name': 'meta-llama/Llama-3.3-70B-Instruct-Turbo-Free', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-070f7431-7176-42a8-ab47-ed83657c9463-0', usage_metadata={'input_tokens': 308, 'output_tokens': 7, 'total_tokens': 315, 'input_token_details': {}, 'output_token_details': {}})\n</code></pre> Access to <code>content</code> property to get the string content.</p> <p><pre><code>message.content\n</code></pre> <pre><code>I am a Financial Analyst.\n</code></pre></p>"},{"location":"get_started/local_run/","title":"Build ReactJS UI on local","text":"<p>TODO</p>"},{"location":"get_started/observability/","title":"Agent Observability","text":"<p>Vinagent provides a local MLflow dashboard that can be used to visualize the intermediate messsages of each query. Therefore, it is an important feature for debugging.</p> <ul> <li>Engineer can trace the number of tokens, execution time, type of tool, and status of exection.</li> <li>Based on tracked results, Agent developers can indentify inefficient steps, optimize agent components like tools, prompts, agent description, agent skills, and LLM model.</li> <li>Accelerate process of debugging and improving the agent's performance.</li> </ul> <pre><code>%pip install vinagent\n</code></pre>"},{"location":"get_started/observability/#start-mlflow-ui","title":"Start MLflow UI","text":"<p>MLflow offers an local UI, which connets to mlflow server understreaming. This UI comprises all experients from conversations between user and agent. To start this UI, let's run this command on <code>terminal/command line interface</code> in your computer:</p> <pre><code>mlflow ui\n</code></pre> <p>An MLflow dashboard starts, which can be accessed at http://localhost:5000.</p>"},{"location":"get_started/observability/#initialize-experiment","title":"Initialize Experiment","text":"<p>Initialize an experiment to auto-log messages for agent</p> <pre><code>import mlflow\nfrom vinagent.mlflow import autolog\n\n# Enable Vinagent autologging\nautolog.autolog()\n\n# Optional: Set tracking URI and experiment\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"agent-dev\")\n</code></pre> <pre><code>&lt;Experiment: artifact_location='mlflow-artifacts:/451007843634367037', creation_time=1751455754824, experiment_id='451007843634367037', last_update_time=1751455754824, lifecycle_stage='active', name='agent-dev', tags={}&gt;\n</code></pre> <p>After this step, an experiment named <code>agent-dev</code> is initialized. An observability and tracing feature are automatically registered for each query to the agent without requiring any changes to the original invocation code.</p>"},{"location":"get_started/observability/#observability-and-tracing","title":"Observability and Tracing","text":"<p>A default MLflow dashboard is launched to display the experiment results, within the Jupyter Notebook, making it convenient for agent developers to test and optimize their agent design directly. Every query is now tracked under the experiment named <code>agent-dev</code>.</p> <pre><code>%%writefile .env\nTOGETHER_API_KEY=\"Your together API key\"\nTAVILY_API_KEY=\"Your Tavily API key\"\n</code></pre> <pre><code>from langchain_together import ChatTogether \nfrom vinagent.agent.agent import Agent\nfrom dotenv import load_dotenv\nload_dotenv()\n\nllm = ChatTogether(\n    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\"\n)\n\nagent = Agent(\n    description=\"You are an Expert who can answer any general questions.\",\n    llm = llm,\n    skills = [\n        \"Searching information from external search engine\\n\",\n        \"Summarize the main information\\n\"],\n    tools = ['vinagent.tools.websearch_tools'],\n    tools_path = 'templates/tools.json',\n    memory_path = 'templates/memory.json'\n)\n\nresult = agent.invoke(query=\"What is the weather today in Ha Noi?\")\n</code></pre> <p>Note</p> <p>You are able to access the dashboard at http://localhost:5000/ and view logs of aformentioned query by accessing to <code>agent-dev</code> and click to <code>Traces</code> tab on the last of header navigation bar of <code>agent-dev</code> experiment.</p> Collapse MLflow Trace"},{"location":"get_started/streaming/","title":"Streaming Agent","text":""},{"location":"get_started/streaming/#install-libraries","title":"Install libraries","text":"<pre><code>%pip install vinagent\n</code></pre>"},{"location":"get_started/streaming/#streaming","title":"Streaming","text":"<p>In addition to synchronous and asynchronous invocation, <code>Vinagent</code> also supports streaming invocation. This means that the response is generated in real-time on token-by-token basis, allowing for a more interactive and responsive experience. To use streaming, simply use <code>agent.stream</code>.</p> <p>Setup environment variables:</p> <pre><code>%%writefile .env\nTOGETHER_API_KEY=\"Your together API key\"\nTAVILY_API_KEY=\"Your Tavily API key\"\n</code></pre> <p>Initialize LLM and Agent:</p> <pre><code>from vinagent.agent.agent import Agent\nfrom langchain_together import ChatTogether\nfrom dotenv import load_dotenv, find_dotenv\n\nload_dotenv(find_dotenv('.env'))\n\n# Step 1: Initialize LLM\nllm = ChatTogether(\n    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\"\n)\n\n# Step 2: Initialize Agent\nagent = Agent(\n    description=\"You are a Weather Analyst\",\n    llm = llm,\n    skills = [\n        \"Update weather at anywhere\",\n        \"Forecast weather in the futher\",\n        \"Recommend picnic based on weather\"\n    ],\n    tools=['vinagent.tools.websearch_tools'],\n    tools_path = 'templates/tools.json', # Place to save tools. Default is 'templates/tools.json'\n    is_reset_tools = True # If True, it will reset tools every time reinitializing an agent. Default is False\n)\n</code></pre> <p>Streaming provides a significant advantage in Agent invocation by delivering output token-by-token in runtime, allowing users to read a long-running answer as it exposures without waiting for the entire response to complete. </p> <ul> <li> <p>It greatly enhances the user experience, especially when integrating the agent into websites or mobile apps, where responsiveness and interactivity are critical. </p> </li> <li> <p>Streaming is particularly effective for long outputs and I/O-bound tasks, enabling dynamic UI updates, early interruption, and a more natural, real-time interaction flow. </p> </li> </ul> <p>You can conveniently use streaming in Vinagent by iterating over the generator returned by the <code>agent.stream()</code> method.</p> <pre><code>content = ''\nfor chunk in agent.stream(query=\"What is the weather in New York today?\"):\n    content += chunk.content\n    content += '|'\n    print(content)\n</code></pre> <pre><code>INFO:vinagent.agent.agent:I am chatting with unknown_user\nINFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n\n\nTo|\n\n\nINFO:vinagent.agent.agent:Tool call: {'tool_name': 'search_api', 'tool_type': 'module', 'arguments': {'query': 'New York weather today'}, 'module_path': 'vinagent.tools.websearch_tools'}\n\n\nTo| find|\nTo| find| the|\nTo| find| the| current|\nTo| find| the| current| weather|\nTo| find| the| current| weather| in|\nTo| find| the| current| weather| in| New|\nTo| find| the| current| weather| in| New| York|\nTo| find| the| current| weather| in| New| York|,|\nTo| find| the| current| weather| in| New| York|,| I|\nTo| find| the| current| weather| in| New| York|,| I| will|\nTo| find| the| current| weather| in| New| York|,| I| will| use|\nTo| find| the| current| weather| in| New| York|,| I| will| use| the|\nTo| find| the| current| weather| in| New| York|,| I| will| use| the| search|\nTo| find| the| current| weather| in| New| York|,| I| will| use| the| search|_api|\nTo| find| the| current| weather| in| New| York|,| I| will| use| the| search|_api| tool|\n\nAccording to the search_api tool, the current weather in New York today is 72\u00b0F with mist. The wind is blowing at 6 mph from the west, and the humidity is relatively high at 94%.|\n</code></pre>"},{"location":"guides/features/","title":"Core Features","text":""},{"location":"guides/features/#tool-integration","title":"Tool integration","text":"<p>https://github.com/datascienceworld-kan/vinagent/tree/main#3-register-tool</p> <p>Vinagent stands out for its flexibility in registering different types of tools, including:</p> <ul> <li>Function tools: These are integrated directly into your runtime code using the @function_tool decorator, without the need to store them in separate Python module files.</li> <li>Module tools: These are added via Python module files placed in the vinagent.tools directory. Once registered, the modules can be imported and used in your runtime environment.</li> <li>MCP tools: These are tools registered through an MCP (Model Context Protocol) server, enabling external tool integration.</li> </ul>"},{"location":"guides/features/#function-tool","title":"Function Tool","text":"<p>You can customize any function in your runtime code as a powerful tool by using the @function_tool decorator.</p> <p><pre><code>from vinagent.register.tool import function_tool\nfrom typing import List\n\n@agent.function_tool # Note: agent must be initialized first\ndef sum_of_series(x: List[float]):\n    return f\"Sum of list is {sum(x)}\"\n</code></pre> <pre><code>INFO:root:Registered tool: sum_of_series (runtime)\n</code></pre></p> <p><pre><code>message = agent.invoke(\"Sum of this list: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]?\")\nmessage\n</code></pre> <pre><code>ToolMessage(content=\"Completed executing tool sum_of_series({'x': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\", tool_call_id='tool_56f40902-33dc-45c6-83a7-27a96589d528', artifact='Sum of list is 55')\n</code></pre></p>"},{"location":"guides/features/#module-tool","title":"Module Tool","text":"<p>Many complex tools cannot be implemented within a single function. In such cases, organizing the tool as a python module becomes necessary. To support this, <code>vinagent</code> allows tools to be registered via python module files placed in the <code>vinagent.tools</code> directory. This approach makes it easier to manage and execute more sophisticated tasks. Once registered, these modules can be imported and used directly in the runtime environment.</p> <pre><code>from langchain_together import ChatTogether \nfrom vinagent.agent.agent import Agent\nfrom dotenv import load_dotenv\nload_dotenv()\n\nllm = ChatTogether(\n    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\"\n)\n\nagent = Agent(\n    description=\"You are a Web Search Expert\",\n    llm = llm,\n    skills = [\n        \"Search the information from internet\", \n        \"Give an in-deepth report\",\n        \"Keep update with the latest news\"\n    ],\n    tools = ['vinagent.tools.websearch_tools'],\n    tools_path = 'templates/tools.json' # Place to save tools. The default path is also 'templates/tools.json',\n    is_reset_tools = True # If True, will reset tools every time. Default is False\n)\n</code></pre>"},{"location":"guides/features/#mcp-tool","title":"MCP Tool","text":"<p>MCP (model context protocal) is a new AI protocal offfered by Anthropic that allows any AI model to interact with any tools distributed acrooss different platforms. These tools are provided by platform's MCP Server. There are many MCP servers available out there such as <code>google drive, gmail, slack, notions, spotify, etc.</code>, and <code>vinagent</code> can be used to connect to these servers and execute the tools within the agent.</p> <p>You need to start a MCP server first. For example, start with math MCP Server</p> <p><pre><code>cd vinagent/mcp/examples/math\nmcp dev main.py\n</code></pre> <pre><code>\u2699\ufe0f Proxy server listening on port 6277\n\ud83d\udd0d MCP Inspector is up and running at http://127.0.0.1:6274 \ud83d\ude80\n</code></pre></p> <p>Next, you need to register the MCP server in the agent. You can do this by adding the server's URL to the <code>tools</code> list of the agent's configuration.</p> <pre><code>from vinagent.mcp.client import DistributedMCPClient\nfrom vinagent.mcp import load_mcp_tools\nfrom vinagent.agent.agent import Agent\nfrom langchain_together import ChatTogether\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Step 1: Initialize LLM\nllm = ChatTogether(\n    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\"\n)\n\n# Step 2: Initialize MCP client for Distributed MCP Server\nclient = DistributedMCPClient(\n            {\n                \"math\": {\n                    \"command\": \"python\",\n                    # Make sure to update to the full absolute path to your math_server.py file\n                    \"args\": [\"vinagent/mcp/examples/math/main.py\"],\n                    \"transport\": \"stdio\",\n                }\n             }\n        )\nserver_name = \"math\"\n\n# Step 3: Initialize Agent\nagent = Agent(\n    description=\"You are a Trending News Analyst\",\n    llm = llm,\n    skills = [\n        \"You are Financial Analyst\",\n        \"Deeply analyzing financial news\"],\n    tools = ['vinagent.tools.yfinance_tools'],\n    tools_path=\"templates/tools.json\",\n    is_reset_tools=True,\n    mcp_client=client, # MCP Client\n    mcp_server_name=server_name, # MCP Server name to resgister. If not set, all tools from all MCP servers available\n)\n\n# Step 4: Register mcp_tools to agent\nmcp_tools = await agent.connect_mcp_tool()\n</code></pre> <pre><code># Test sum\nagent.invoke(\"What is the sum of 1993 and 709?\")\n</code></pre> <pre><code># Test product\nagent.invoke(\"Let's multiply of 1993 and 709?\")\n</code></pre>"},{"location":"guides/features/#inference","title":"Inference","text":""},{"location":"guides/features/#synchronous-and-asynchronous","title":"Synchronous and Asynchronous","text":"<p>Vinagent offers both synchronous (agent.invoke) and asynchronous (agent.ainvoke) invocation methods. While synchronous calls halt the main thread until a response is returned, asynchronous calls enable the main thread to proceed without waiting. In practice, asynchronous invocations can be up to twice as fast as synchronous ones.</p> <pre><code># Synchronous invocation\nmessage = agent.invoke(\"What is the sum of 1993 and 709?\")\n</code></pre> <pre><code># Asynchronous invocation\nmessage = await agent.ainvoke(\"What is the sum of 1993 and 709?\")\n</code></pre>"},{"location":"guides/features/#streaming","title":"Streaming","text":"<p>In addition to synchronous and asynchronous invocation, vinagent also supports streaming invocation. This means that the response is generated in real-time on token-by-token basis, allowing for a more interactive and responsive experience. To use streaming, simply use <code>agent.stream</code>:</p> <pre><code>for chunk in agent.stream(\"Where is the capital of the Vietnam?\"):\n    print(chunk)\n</code></pre>"},{"location":"guides/features/#advanced-features","title":"Advanced Features","text":""},{"location":"guides/features/#deep-search","title":"Deep Search","text":"<p>With vinagent, you can invent a complex workflow by combining multiple tools into a single agent. This allows you to create a more sophisticated and flexible agent that can adapt to different task. Let's see how an agent can be created to help with financial analysis by using <code>deepsearch</code> tool, which allows you to search for information in a structured manner. This tool is particularly useful for tasks that require a deep understanding of the data and the ability to navigate through complex information.</p> <pre><code>from langchain_together import ChatTogether \nfrom vinagent.agent import Agent\nfrom dotenv import load_dotenv\nload_dotenv()\n\nllm = ChatTogether(\n    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\"\n)\n\nagent = Agent(\n    description=\"You are a Financial Analyst\",\n    llm = llm,\n    skills = [\n        \"Deeply analyzing financial markets\", \n        \"Searching information about stock price\",\n        \"Visualization about stock price\"],\n    tools = ['vinagent.tools.deepsearch']\n)\n\nmessage = agent.invoke(\"Let's analyze Tesla stock in 2025?\")\nprint(message.artifact)\n</code></pre> <p></p> <p>The output is available at vinagent/examples/deepsearch.md</p>"},{"location":"guides/features/#trending-search","title":"Trending Search","text":"<p>Exceptionally, vinagent also offers a feature to summarize and highlight the top daily news on the internet based on any topic you are looking for, regardless of the language used. This is achieved by using the <code>trending_news</code> tool.</p> <pre><code>from langchain_together import ChatTogether \nfrom vinagent.agent.agent import Agent\nfrom dotenv import load_dotenv\nload_dotenv()\n\nllm = ChatTogether(\n    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\"\n)\n\nagent = Agent(\n    description=\"You are a Trending News Analyst\",\n    llm = llm,\n    skills = [\n        \"Searching the trending news on realtime from google news\",\n        \"Deeply analyzing top trending news\"],\n    tools = ['vinagent.tools.trending_news']\n)\n\nmessage = agent.invoke(\"T\u00ecm 5 tin t\u1ee9c n\u1ed5i b\u1eadt v\u1ec1 t\u00ecnh h\u00ecnh gi\u00e1 v\u00e0ng s\u00e1ng h\u00f4m nay\")\nprint(message.artifact)\n</code></pre> <p></p> <p>The output is available at vinagent/examples/todaytrend.md</p>"},{"location":"guides/features/#integrate-with-memory","title":"Integrate with memory","text":"<p>There is a special feature that allows to adhere Memory for each Agent. This is useful when you want to keep track of the user's behavior and conceptualize them as a knowledge graph for the agent. Therefore, it helps agent become more intelligent and capable of understanding personality and responding to user queries with greater accuracy.</p> <p>The following code to save each conversation into short-memory.</p> <pre><code>from langchain_together.chat_models import ChatTogether\nfrom dotenv import load_dotenv\nfrom vinagent.memory import Memory\n\nload_dotenv()\n\nllm = ChatTogether(\n    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\"\n)\n\nmemory = Memory(\n    memory_path=\"templates/memory.jsonl\",\n    is_reset_memory=True, # will reset the memory every time the agent is invoked\n    is_logging=True\n)\n\ntext_input = \"\"\"Hi, my name is Kan. I was born in Thanh Hoa Province, Vietnam, in 1993.\nMy motto is: \"Make the world better with data and models\". That\u2019s why I work as an AI Solution Architect at FPT Software and as an AI lecturer at NEU.\nI began my journey as a gifted student in Mathematics at the High School for Gifted Students, VNU University, where I developed a deep passion for Math and Science.\nLater, I earned an Excellent Bachelor's Degree in Applied Mathematical Economics from NEU University in 2015. During my time there, I became the first student from the Math Department to win a bronze medal at the National Math Olympiad.\nI have been working as an AI Solution Architect at FPT Software since 2021.\nI have been teaching AI and ML courses at NEU university since 2022.\nI have conducted extensive research on Reliable AI, Generative AI, and Knowledge Graphs at FPT AIC.\nI was one of the first individuals in Vietnam to win a paper award on the topic of Generative AI and LLMs at the Nvidia GTC Global Conference 2025 in San Jose, USA.\nI am the founder of DataScienceWorld.Kan, an AI learning hub offering high-standard AI/ML courses such as Build Generative AI Applications and MLOps \u2013 Machine Learning in Production, designed for anyone pursuing a career as an AI/ML engineer.\nSince 2024, I have participated in Google GDSC and Google I/O as a guest speaker and AI/ML coach for dedicated AI startups.\n\"\"\"\n\nmemory.save_short_term_memory(llm, text_input)\nmemory_message = memory.load_memory('string')\nmemory_message\n</code></pre> <pre><code>Kan -&gt; BORN_IN[in 1993] -&gt; Thanh Hoa Province, Vietnam\nKan -&gt; WORKS_FOR[since 2021] -&gt; FPT Software\nKan -&gt; WORKS_FOR[since 2022] -&gt; NEU\nKan -&gt; STUDIED_AT -&gt; High School for Gifted Students, VNU University\nKan -&gt; STUDIED_AT[graduated in 2015] -&gt; NEU University\nKan -&gt; RESEARCHED_AT -&gt; FPT AIC\nKan -&gt; RECEIVED_AWARD[at Nvidia GTC Global Conference 2025] -&gt; paper award on Generative AI and LLMs\nKan -&gt; FOUNDED -&gt; DataScienceWorld.Kan\nKan -&gt; PARTICIPATED_IN[since 2024] -&gt; Google GDSC\nKan -&gt; PARTICIPATED_IN[since 2024] -&gt; Google I/O\nDataScienceWorld.Kan -&gt; OFFERS -&gt; Build Generative AI Applications\nDataScienceWorld.Kan -&gt; OFFERS -&gt; MLOps \u2013 Machine Learning in Production\nKan -&gt; OWNS -&gt; house\nKan -&gt; OWNS -&gt; garden\nKan -&gt; HAS_MOTTO -&gt; stay hungry and stay foolish\nKan -&gt; RECEIVED_AWARD_AT[in 2025] -&gt; Nvidia GTC Global Conference\n</code></pre> <p>To adhere Memmory to each Agent</p> <pre><code>import os\nimport sys\nfrom langchain_together import ChatTogether \nfrom vinagent.agent import Agent\nfrom vinagent.memory.memory import Memory\nfrom pathlib import Path\nfrom dotenv import load_dotenv\nload_dotenv()\n\nllm = ChatTogether(\n    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\"\n)\n\n# Step 1: Create Agent with tools\nagent = Agent(\n    llm = llm,\n    description=\"You are my close friend\",\n    skills=[\n        \"You can remember all memory related to us\",\n        \"You can remind the memory to answer questions\",\n        \"You can remember the history of our relationship\"\n    ],\n    memory_path='templates/memory.json',\n    is_reset_memory=True # Will reset memory each time re-initialize agent. Default is False\n)\n\n# Step 2: invoke the agent\nmessage = agent.invoke(\"Hello how are you?\")\nmessage.content\n</code></pre>"},{"location":"guides/features/#design-agents-workflow","title":"Design Agent's workflow","text":"<p>The Vinagent library enables the integration of workflows built upon the nodes and edges of LangGraph. What sets it apart is our major improvement in representing a LangGraph workflow through a more intuitive syntax for connecting nodes using the right shift operator (<code>&gt;&gt;</code>). All agent patterns such as ReAct, chain-of-thought, and reflection can be easily constructed using this simple and readable syntax.</p> <p>We support two styles of creating a workflow: - <code>FlowStateGraph</code>: Create nodes by concrete class nodes inherited from class Node of vinagent. - <code>FunctionStateGraph</code>: Create a workflow from function, which are decorated with @node to convert this function as a node. </p>"},{"location":"guides/features/#flowstategraph","title":"FlowStateGraph","text":"<p>These are steps to create a workflow:</p> <ol> <li> <p>Define General Nodes Create your workflow nodes by inheriting from the base Node class. Each node typically implements two methods:</p> </li> <li> <p><code>exec</code>: Executes the task associated with the node and returns a partial update to the shared state.</p> </li> <li> <p><code>branching</code> (optional): For conditional routing. It returns a string key indicating the next node to be executed.</p> </li> <li> <p>Connect Nodes with <code>&gt;&gt;</code> Operator Use the right shift operator (<code>&gt;&gt;</code>) to define transitions between nodes. For branching, use a dictionary to map conditions to next nodes.</p> </li> </ol> <pre><code>from typing import Annotated, TypedDict\nfrom vinagent.graph.operator import FlowStateGraph, END, START\nfrom vinagent.graph.node import Node\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.utils.runnable import coerce_to_runnable\n\n# Define a reducer for message history\ndef append_messages(existing: list, update: dict) -&gt; list:\n    return existing + [update]\n\n# Define the state schema\nclass State(TypedDict):\n    messages: Annotated[list[dict], append_messages]\n    sentiment: str\n\n# Optional config schema\nclass ConfigSchema(TypedDict):\n    user_id: str\n\n# Define node classes\nclass AnalyzeSentimentNode(Node):\n    def exec(self, state: State) -&gt; dict:\n        message = state[\"messages\"][-1][\"content\"]\n        sentiment = \"negative\" if \"angry\" in message.lower() else \"positive\"\n        return {\"sentiment\": sentiment}\n\n    def branching(self, state: State) -&gt; str:\n        return \"human_escalation\" if state[\"sentiment\"] == \"negative\" else \"chatbot_response\"\n\nclass ChatbotResponseNode(Node):\n    def exec(self, state: State) -&gt; dict:\n        return {\"messages\": {\"role\": \"bot\", \"content\": \"Got it! How can I assist you further?\"}}\n\nclass HumanEscalationNode(Node):\n    def exec(self, state: State) -&gt; dict:\n        return {\"messages\": {\"role\": \"bot\", \"content\": \"I'm escalating this to a human agent.\"}}\n\n# Define the Agent with graph and flow\nclass Agent:\n    def __init__(self):\n        self.checkpoint = MemorySaver()\n        self.graph = FlowStateGraph(State, config_schema=ConfigSchema)\n        self.analyze_sentiment_node = AnalyzeSentimentNode()\n        self.human_escalation_node = HumanEscalationNode()\n        self.chatbot_response_node = ChatbotResponseNode()\n\n        self.flow = [\n            self.analyze_sentiment_node &gt;&gt; {\n                \"chatbot_response\": self.chatbot_response_node,\n                \"human_escalation\": self.human_escalation_node\n            },\n            self.human_escalation_node &gt;&gt; END,\n            self.chatbot_response_node &gt;&gt; END\n        ]\n\n        self.compiled_graph = self.graph.compile(checkpointer=self.checkpoint, flow=self.flow)\n\n    def invoke(self, input_state: dict, config: dict) -&gt; dict:\n        return self.compiled_graph.invoke(input_state, config)\n\n# Test the agent\nagent = Agent()\ninput_state = {\n    \"messages\": {\"role\": \"user\", \"content\": \"I'm really angry about this!\"}\n}\nconfig = {\"configurable\": {\"user_id\": \"123\"}, \"thread_id\": \"123\"}\nresult = agent.invoke(input_state, config)\nprint(result)\n</code></pre> <p>Output:</p> <pre><code>{\n  'messages': [\n    {'role': 'user', 'content': \"I'm really angry about this!\"},\n    {'role': 'bot', 'content': \"I'm escalating this to a human agent.\"}\n  ],\n  'sentiment': 'negative'\n}\n</code></pre> <p>We can visualize the graph workflow on jupyternotebook</p> <pre><code>agent.compiled_graph\n</code></pre> <p></p>"},{"location":"guides/features/#functionstategraph","title":"FunctionStateGraph","text":"<p>We can simplify the coding style of a graph by converting each function into a node and assigning it a name.</p> <ol> <li> <p>Each node will be a function with the same name as the node itself. However, you can override this default by using the <code>@node(name=\"your_node_name\")</code> decorator.</p> </li> <li> <p>If your node is a conditionally branching node, you can use the <code>@node(branching=fn_branching)</code> decorator, where <code>fn_branching</code> is a function that determines the next node(s) based on the return value of current state of node.</p> </li> <li> <p>In the Agent class constructor, we define a flow as a list of routes that connect these node functions.</p> </li> </ol> <pre><code>from typing import Annotated, TypedDict\nfrom vinagent.graph.operator import END, START\nfrom vinagent.graph.function_graph import node, FunctionStateGraph\nfrom vinagent.graph.node import Node\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.utils.runnable import coerce_to_runnable\n\n# Define a reducer for message history\ndef append_messages(existing: list, update: dict) -&gt; list:\n    return existing + [update]\n\n# Define the state schema\nclass State(TypedDict):\n    messages: Annotated[list[dict], append_messages]\n    sentiment: str\n\n# Optional config schema\nclass ConfigSchema(TypedDict):\n    user_id: str\n\ndef branching(state: State) -&gt; str:\n    return \"human_escalation\" if state[\"sentiment\"] == \"negative\" else \"chatbot_response\"\n\n@node(branching=branching, name='AnalyzeSentiment')\ndef analyze_sentiment_node(state: State) -&gt; dict:\n    message = state[\"messages\"][-1][\"content\"]\n    sentiment = \"negative\" if \"angry\" in message.lower() else \"positive\"\n    return {\"sentiment\": sentiment}\n\n@node(name='ChatbotResponse')\ndef chatbot_response_node(state: State) -&gt; dict:\n    return {\"messages\": {\"role\": \"bot\", \"content\": \"Got it! How can I assist you further?\"}}\n\n@node(name='HumanEscalation')\ndef human_escalation_node(state: State) -&gt; dict:\n    return {\"messages\": {\"role\": \"bot\", \"content\": \"I'm escalating this to a human agent.\"}}\n\n# Define the Agent with graph and flow\nclass Agent:\n    def __init__(self):\n        self.checkpoint = MemorySaver()\n        self.graph = FunctionStateGraph(State, config_schema=ConfigSchema)\n\n        self.flow = [\n            analyze_sentiment_node &gt;&gt; {\n                \"chatbot_response\": chatbot_response_node,\n                \"human_escalation\": human_escalation_node\n            },\n            human_escalation_node &gt;&gt; END,\n            chatbot_response_node &gt;&gt; END\n        ]\n\n        self.compiled_graph = self.graph.compile(checkpointer=self.checkpoint, flow=self.flow)\n\n    def invoke(self, input_state: dict, config: dict) -&gt; dict:\n        return self.compiled_graph.invoke(input_state, config)\n\n# Test the agent\nagent = Agent()\ninput_state = {\n    \"messages\": {\"role\": \"user\", \"content\": \"I'm really angry about this!\"}\n}\nconfig = {\"configurable\": {\"user_id\": \"123\"}, \"thread_id\": \"123\"}\nresult = agent.invoke(input_state, config)\nprint(result)\n</code></pre> <p>Output:</p> <pre><code>{'messages': [{'role': 'user', 'content': \"I'm really angry about this!\"}, {'role': 'bot', 'content': \"I'm escalating this to a human agent.\"}], 'sentiment': 'negative'}\n</code></pre> <p>Visualize the workflow:</p> <pre><code>agent.compiled_graph\n</code></pre> <p></p>"},{"location":"guides/features/#observability-and-monitoring","title":"Observability and monitoring","text":"<p>Vinagent provides a local server that can be used to visualize the intermediate messsages of Agent's workflow to debug. Engineer can trace the number of tokens, execution time, type of tool, and status of exection. We leverage mlflow observability to track the agent's progress and performance. To use the local server, run the following command:</p> <ul> <li>Step 1: Start the local mlflow server.</li> </ul> <p><pre><code>mlflow ui\n</code></pre> This command will deploy a local loging server on port 5000 to your agent connect to.</p> <ul> <li>Step 2: Initialize an experiment to auto-log messages for agent</li> </ul> <pre><code>import mlflow\nfrom vinagent.mlflow import autolog\n\n# Enable Vinagent autologging\nautolog.autolog()\n\n# Optional: Set tracking URI and experiment\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"Vinagent\")\n</code></pre> <p>After this step, one hooking function will be registered after agent invoking.</p> <ul> <li>Step 3: Run your agent</li> </ul> <pre><code>from langchain_together import ChatTogether \nfrom vinagent.agent.agent import Agent\nfrom dotenv import load_dotenv\nload_dotenv()\n\nllm = ChatTogether(\n    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\"\n)\n\nagent = Agent(\n    description=\"You are an Expert who can answer any general questions.\",\n    llm = llm,\n    skills = [\n        \"Searching information from external search engine\\n\",\n        \"Summarize the main information\\n\"],\n    tools = ['vinagent.tools.websearch_tools'],\n    tools_path = 'templates/tools.json',\n    memory_path = 'templates/memory.json'\n)\n\nresult = agent.invoke(query=\"What is the weather today in Ha Noi?\")\n</code></pre> <p>An experiment dashboard of Agent will be available on your <code>jupyter notebook</code> for your observability. If you run code in terminal environment, you can access the dashboard at <code>http://localhost:5000/</code> and view experiment <code>Vinagent</code> at tracing tab. Watch the following video to learn more about Agent observability feature:</p> <p></p>"},{"location":"guides/api/agent/","title":"Agent","text":"<ul> <li>What is agent</li> <li>How does it work?</li> </ul>"},{"location":"guides/api/flow/","title":"Flow","text":""},{"location":"guides/api/flow/#agent-flow","title":"Agent Flow","text":"<ul> <li>What is Flow</li> <li>How does it work?</li> </ul>"},{"location":"guides/api/llm/","title":"Llm","text":""},{"location":"guides/api/llm/#llm","title":"LLM","text":"<ul> <li>What is LLM</li> <li>How does it work?</li> </ul>"},{"location":"guides/api/mcp/","title":"Mcp","text":""},{"location":"guides/api/mcp/#mcp","title":"MCP","text":"<ul> <li>What is MCP</li> <li>How does it work?</li> </ul>"},{"location":"guides/api/memory/","title":"Memory","text":""},{"location":"guides/api/memory/#memory","title":"Memory","text":"<ul> <li>What is memory</li> <li>How does it work?</li> </ul>"},{"location":"guides/api/observability/","title":"Observability","text":""},{"location":"guides/api/observability/#observability","title":"Observability","text":"<ul> <li>What is observability</li> <li>How does it work?</li> </ul>"},{"location":"reference/api/","title":"API","text":""},{"location":"usage/usage/","title":"Usage","text":""}]}